---
title: 普林斯顿 概率论读本
date: 2025-01-01
categories:
  - 概率论
tags:
  - 概率论
---


前置知识：微积分， 看了下有些结论需要依赖复分析的结果

isbn 9787115543776

优点：
- 脉络很好，非常喜欢整体性，

缺点：
- 有习题 无直接的答案，好像也没有答案书
- 很多专业名词出现时没有对应英文

<!--more-->

## 第一部分 一般性理论

### 1.引言

生日问题，房间里有多少人才能保证，两人同生日概率 >= 50%, 假设365天，如果一个人在一年中出生概率相等（然而似乎统计的现实不是这样，而现实中一个小圈圈可能也有生日聚集性）
- 一个直接的错误的 想法是 182.5,
- $=\frac{365!}{365^n(365-n)!}$
	- 23人时达到 ~50.7%
	- 30人时达到 ~70.6%
	- 40人时达到 ~89.1%
	- 50人时达到 ~97%
- d天内分布，大约需要 sqrt{d 2 log 2}个人，使得 同一天发生概率50%

只要看到乘积就取其对数，巴普洛夫条件反射

$log p_n=\sum_{k=0}^{n-1} log(1-\frac{k}{d})=\sum_{k=0}^{n-1} -\frac{k}{d}=-\frac{n(n-1)}{2d}$ 这里用了k远小于d, 等差数列求和

$n(n-1)=2d \log 2$, 继续大数 $n=\sqrt{d 2 \log 2}$

最后是程序暴力模拟检验一下是否严重偏差

---

先p成功，1-p失败，后手q成功1-q失败，一人成功时结束游戏，问先手成功的概率
- 按次数统计 =p+(1-p)(1-q)p+(1-p)^2(1-q)^2p+.....
	- $=p\frac{1}{1-(1-p)(1-q)}$

---

代回法的例子，感觉核心需要级数收敛，例子 sum Fib_n/3^n,  这里用概率论的描述法举例了，但是要注意描述法的一致性

结尾给了个可以二次下注，且期望恒正的对冲例子
### 2.基本概率定律

罗素悖论表明：依靠直觉来判断该如何使用集合会给我们带来麻烦

$R=${$x|x\not\in x$}

我们应该意识到，松散定义和非正式论述的危险

---

集合论综述，属于，子集，超集，空集，并，交，补集，笛卡尔乘积
- 常用集合相等证明方法：互相包含
- 这里只讨论可数事件的概率

开集，闭集

---

希望：
- 概率$P(A) \in[0,1]$
- $P(结果空间) = 1$
- 任意多个不相交事件$A_i$, $P(A_i的并)=\sum P(A_i)$

柯尔莫戈洛夫的 概率公理：$\Omega$是一个结果空间,$\Sigma$是一个$\sigma$代数。如果概率函数满足下列条件，那么$(\Omega,\Sigma,Prob)$就是一个概率空间
- 若$A\in\Sigma$，那么$Pr(A)$是有定义的，且$0\le Pr(A)\le 1$
- $Pr(\emptyset)=0,Pr(\Omega)=1$
- {$A_i$}有限个两两互不相交的集合构成的集族，并且每一个集合都是$\Omega$中的元素，那么$Pr_i(\cup A_i)=\sum_i Pr(A_i)$

概率空间有用的规则
- $A\in \Omega$则$Pr(A)+Pr(A^c)=1$, 其中$A^c$为补集
- $Pr(A\cup B)=Pr(A)+Pr(B)-Pr(A\cap B)$ 这个还可以延伸更多项，从而有容斥原理
- $A\subset B$则 $Pr(A)\le Pr(B)$,以及$Pr(B)=Pr(A)+Pr(B\cap A^c)$ 这里注意是 拆出来的部分可能概率为0
	- 对上面这条推广，$A_i \subset B$ 有$Pr(\cup A_i)\le Pr(B)$

对于给定的并集，通常希望能把它写成几个互不相交的集合的并

---

概率空间和$\sigma$代数

巴拿赫-塔尔斯基悖论： 利用（不可数的）选择公理
- 分球
- $x-y\in Q$ 是有理数，那么看作等价类,$[x]=${$y\in[0,1]|x\sim y$}
- 利用选择公理$[0,1]$中 每个等价类中取元素组成集合$A$, (或者说 $A=[0,1]/\sim$ 的商集)
- 对于有理数r,令 $A_r=${$x+r|x\in A$}$\cap [0,1]$, 不同有理数 的 交集=空集,
- $[0,1] = \cup_{r\in Q} A_r$
- 这里 无法对A分配概率


结果空间是 半径1的圆内，那么Pr(一个点)的概率是多少呢？，会发现任何值都不对
- Pr(点)=0 那么总概率0
- Pr(点) = c > 0,那么总概率无穷大

$\sigma$代数 定义：
- $\Omega$是集合，$\Sigma$是$\Omega$子集构成的一个非空集合，满足下面前提，则是$\sigma$代数
	- 若$A\in \Sigma$则$A^c\in \Sigma$
	- $\Sigma$的子集的“可数并”仍然属于$\Sigma$, $A_i$均满足 $A_i\in\Sigma$ 那么$\cup A_i = \Sigma$
	- 把$\Omega$中的元素称为事件、

柯尔莫戈洛夫的 概率公理：$\Sigma$是结果空间$\Omega$的一个$\sigma$代数，我们可以定义一个概率函数Prob: $\Sigma \to[0,1]$
- 可以为$\Sigma$中满足以下性质的每个元素分配一个0~1之间的概率
- $A\in \Sigma$ $Pr(A)\in [0,1]$
- $Pr(\Omega)=1$
- 互不交的$A_i$有$Pr(\cup A_i)=\sum Pr(A_i)$， 这里就是要可数并

再看上面的圆的问题，1=Pr(落在圆上)=?=\sum_所有点 Pr(落在点上)=0?
- 这里和上面定义的关键就是 所有点 这里是不可数的，不满足上面的要求
- 所以 Pr(落在圆上)=1,和Pr(落在一个具体点上)=0 都是对的

### 3.计数I： 纸牌

对于坚持要赌博的人，请阅读第23章

阶乘: n! 叙事解释成n人排序的方案数
- 延伸 gamma函数

二项式系数：binom(n,k)=n!/(k!(n-k)!), 解释n个人中选k个不考虑顺序的方案
- 延伸 广义二项式负数
- $\binom{n}{k}+\binom{n}{k+1}=\binom{n+1}{k+1}$

---

作者列举了各种扑克牌游戏

### 4. 条件概率、独立性和贝叶斯定理

条件概率：在已知其他事件发生的前提下，我们想知道某个时间发生的概率有多大

帽子游戏概率论版：3个人，可游玩前制定策略，进入游戏后禁止交流，每个人白黑是独立1/2概率决定，每个人仅能看到其它两人帽子，不可交流，每人猜测或弃权，所有人同时进行，当猜测人数非零，且参与猜测全正确，平分奖金，有什么策略能大于50%
- 显然一个策略 是其中一个人钦定白色，另外两个人不发言 是50%

条件概率： P(A|B)=在B发生时A的概率
- $P(A|B)=P(A\cap B)/(P(A\cap B)+P(A^c\cap B))=P(A\cap B)/P(B)$
- 蒙提霍尔开门问题

一般乘法$P(A\cap B)=P(A|B)\cdot P(B)$

---

独立性

A和B相互独立: $P(A|B)=P(A)$
- B的发生不会改变A的概率
- $P(A\cap B)=P(A)\cdot P(B)$
	- n个
	- $P(\cap A_i)=\prod P(A_i)$ 且，{A_i} 任意一个非空子集都是相互独立的

---

贝叶斯定理：$P(A|B)=P(B|A)\frac{P(A)}{P(B)}$
- 我还是喜欢之前看到的 P(高学历|有礼貌)
- $P(A)=P(A|B)P(B)+P(A|B^c)P(B^c)$
- 划分：两两事件交为空，所有事件并=整个样本空间
	- 对于上面的扩展到n个事件 $P(A)=\sum P(A|B_i)P(B_i)$
- $P(A|B)=\frac{P(B|A)P(A)}{\sum P(B|A_i)P(A_i)}$, 其中{$A_i$}是划分

### 5. 计数II: 容斥原理

n个事件A_i,例如容易算交，不容易算并

$P(\cup A_i)=-\sum_{t=1}^n (-1)^t \sum_{遍历t个事件选择} P(\cap A_j)$
- 简单的理解，就是看n=2,3时的表达式
- 证明 对于一个元素，恰好属于上面k > 0 个A_i,那么考虑上面表达式这个元素被统计的次数 $-\sum_{t=1}^{k} (-1)^t \binom{k}{t}=1-(-1+1)^t=1$

算法竞赛里经常用到

---

N(K)表示至少K个
- E(K)恰好K个 = N(K)-N(K+1)
- 这个解决了 一些没有直接设容斥的

例子：错排概率，n个元素，所有元素都不在它原来位置上的概率
- 没法直接 事件
- N(K)=至少K个在原来位置上概率
- E(0)=N(0)-N(1)=n!-N(1)
- A_i=第i个元素在原来位置上, $N(1)=P(\cup A_i)$ 可以容斥
- $D_n=n!-\frac{n!}{1!}+\frac{n!}{2!}-\frac{n!}{3!}+\cdots+(-1)^n\frac{n!}{n!}$
- $\frac{D_n}{n!} \sim e^{-1}$


### 6. 计数III高等组合学

不同重排序，binom的扩展
- $\frac{N!}{n_1!n_2!\cdots n_k!}$

隔板法：binom扩展，c个相同饼干分给p个不同人 $\binom{c+p-1}{p-1}$

## 第二部分 随机变量

### 7. 离散型随机变量

离散型随机变量X就是定义在一个离散的结果空间$\Omega$ 这意味着（$\Omega$是有限的或至多可数的）上的实值函数，具体地说，我们为每个元素指定一个$X(\omega)$ 实数

probability density function 概率密度函数
- 离散型概率密度函数，X随机变量，它定义在离散的结果空间 $\Omega$上  $f_X(x)=Prob(\omega \in \Omega: X(\omega)=x)$,
- 关于 随机变量X的 概率密度函数，的=x时的值，就是X恰好取x的概率。 也就是原来有Prob,现在基于prob定义了$f_X(x)$

cumulative distribution function 累计分布函数
- 区别是 对 随机变量不超过x的 概率的汇总 $F_x(x)=Prob(\omega \in \Omega: X(\omega) \le x)$

这里例子是n个硬币，正面次数和的概率, 二项式分布
- $f_X(k)=\binom{n}{k}p^{k}(1-p)^{n-k}$
- $F_x(k)=\sum_{i=0}^k \binom{n}{i}/2^n$
- 这里教材上，还扩展了定义域到 其它，并不是限死在0~n? 往后看，那么定义 非可取点就有一定的合理性了？
	- 离散型 累计分布函数 的极限：$\lim_{x\to - \infty}F_X(x)=0,\lim_{x\to \infty}F_X(x)=1$, 且非严格单调递增

### 8. 连续型随机变量

利用 速度，时间，路程 回顾了以下微积分知识
- 稍微提了以下 有限分段，也点了以下实分析和勒贝格积分
- $\int_a^b f(x)dx=F(b)-F(a)$

希望 基于离散随机变量的基础，来建立连续变量的 概率理论

连续型随机变量、概率密度函数和累积 分布函数：X是随机变量，如果存在$f_X$满足
- 分段连续函数 $f_X$
- $f_X(x)\ge 0$
- $\int_{-\infty}^{\infty} f_X(t)dt = 1$

那么$X$是一个连续型随机变量，f_X是X的概率密度函数，X的累积分布函数$F_X(x)$就是X不大于x的概率
- $F_X(x)=Prob(X\le x)=\int_{-\infty}^x f_X(t) dt$
- 非严格单调递增，左极限0,右极限1,

那么微积分基本定理的意义是，概率可以等价于 概率密度函数 下面的面积

例子 $x\in[0,1], f_X(x)=2+3x-5x^2$ 当$x$在其它值时$f_X(x)=0$
- 验证： 分段连续（满足），非负（满足），积分为1(不满足)

对于积分不为0任意非负分段连续函数 $f(x)$. 容易构造$g(x)=\frac{f(x)}{\int_{-\infty}^{\infty} f(x)dx}$ 来完成 第3个条件 积分为1
- 根据微积分的“补充定义”来看， X属于 $[a,b],(a,b],[a,b),(a,b)$ 对于连续型随机变量来说 是相等的

### 9. 工具：期望

这章脉络是：
- 矩能描绘概率密度函数的形状
- 我们希望能 对于 不同随机变量之间 进行线性运算，希望能求得新的随机变量的 概率密度函数性质（也就是矩函数的结果）
- 这里重点在于 一阶矩（均值）和 二阶中心矩（方差）展开

回顾 微积分知识：泰勒展开，x在点a展开，可以估算“附近”的值

期望值和矩(moment)： X是在R上的随机变量，概率密度函数$f_X$, 函数$g(X)$的 期望值 是

- $E[g(X)] =\int_{-\infty}^{\infty} g(x)f_X(x) dx$ 连续型
- $E[g(X)] =\sum_{i} g(x_i)f_X(x_i) dx$ 离散型

特别的
- 当$g(x)=x^r$ 把$E[X^r]$称为X的r阶矩，$E[(X-E[X])^r]$ 称为X的r阶中心矩

显然的问题：为什么要算期望值，能得到什么信息
- 类似泰勒级数，知道更多的矩能让我们更好地理解 概率密度函数 的形状和性质！！！！！？？？？？ 不是一个值吗？ 搜了下知乎，见最底外链
- 之后会聚焦两个 均值（1阶矩）方差（2阶中心矩）

这里给了 $x\in[0,1] f_X(x) = \frac{6}{11}(2+3x-5x^2)$ 的例子
- 对于$g(x)=x^r$,$g(x)=e^x$,$g(x)=1/x$ 分别 做了积分计算，

---

这里写了一边公式，不如直接用上面的定义
- 对于离散连续都是
- 均值 = $E[g(X)=X]$ 
- 方差 $\sigma_X^2=Var(X)$ = $E[g(X)=(X-E[X])^2]$  二阶中心矩
	- 这里 的一些可拆分，是因为 积分和求和的 加减可拆分
	- 注意对于二阶中心矩 `E[X]`是一个值(常数的感觉)，不是随机变量, $E[(X-E[X])^2]=E[X^2-2XE[X]+E[X]^2]=E[X^2]-2E[X]E[X]+E[X]^2=E[X^2]-E[X]^2$
- 标准差$\sigma_X=\sqrt{Var(X)}$
	- 优势是“单位相同”
- 技术说明：为了保证均值存在，希望 运算结果是有限的


一个例子 柯西分布 $f_X(x)=\frac{1}{\pi}\frac{1}{1+x^2}$
- 这个例子说明 了 均值：看起来是 奇函数积分=0,但是 无穷大，选择不同的左右极限逼近方式，得到的结果不同，所以值与趋近方式有关，极限不存在

---

联合分布

联合概率密度函数：$X_i$ 都是连续随机变量，$f_{X_i}$ 是它们的概率密度函数
- 如果 $f_{X_i}(x_i)=\int\int\int f_{X,X,...,X}(x,x,...,x) dx_1 dx_2...$ 也就是 对其它n-1个变量积分得来
- 那么$f_{X_i}$称为$X_i$的边缘密度函数
- 独立：如果满足 $f_{X_1,\cdots,X_n}(x_1,\cdots,x_n)=\prod f_{X_i}(x_i)$
- 这 从形状上理解，可以从二维的看，$z(x,y)=f_{X,Y}(x,y)$, 那么分别的 沿着一个方向 挤压到一起（积分到一起），就得到没有被压的那个维度的边缘密度函数，
- 例子 $f_{X,Y}(x,y)=1/\pi, x^2+y^2\le 1$其它时候为0

---

期望值的线性性质
- 咋这才讲，感觉上面讲期望就可以讲
- 核心就是 多个随机变量满足线性关系 $E[\sum_i a_ig(X_i)]=\sum_i a_i E[g(X_i)]$, 根据连续的积分 离散的求和 的加减法是线性的显然
	- $E[常数c]=c$

引理9.5.2 X是随机变量，均值$\mu_X$,方差Var(X),若 a,b是常数，那么随机变量 Y=aX+b有下列结果
- $\mu_Y=a\mu_X+b$
- 利用线性性质
- $Var(Y)=E[Y^2]-E[Y]^2=E[(aX+b)^2]-E[aX+b]^2=(a^2E[X^2]+2abE[X]+b^2)-(a^2E[X]^2+2abE[X]+b^2)=a^2(E[X^2]-E[X]^2)=a^2Var(X)$
- 另一方面
	- 我们认为 均值 一阶矩函数描绘的概率密度函数的平均值，所以 倍增 平移是同步的
	- 我们认为 方差 二阶中心矩函数 描绘的是到均值的 移动/距离 情况，所以平移是无效的，不会有b的存在

---

均值方差的性质:
- 9.6.1 X,Y是相互独立的随机变量，那么$E[XY]=E[X][Y]$
	- 证明 就是回到 积分里，根据“独立”的定义 联合分布$f_{X,Y}(x,y)$可以拆成两个 边缘密度函数的乘积
- 一种特别重要的情况是 $E[(X-\mu_X)(Y-\mu_Y)]=E[X-\mu_X]E[Y-\mu_Y]=0$

9.6.2： $X_i$是n个随机变量
- 均值 $E[\sum X_i]=\sum E[X_i]$
- 如果随机变量相互独立，那么 $Var(\sum X_i)=\sum Var(X_i)$
	- 这里的核心原理就是 中心矩 中间可以线性拆开，拆开的分配率会出现上面的形如$E[(X-\mu_X)(Y-\mu_y)]$的形式，这个形式在 独立时 为0
		- 例子是 X,和-X不独立，那么和为常数 方差为0
	- 如果独立+同分布, $E[\sum X_i]=nE[X_1]$, $Var(\sum X_i)=nVar(X_i)$


协方差（Variance）： X和Y是两个随机变量。X和Y的协方差记作$\sigma_{XY}$或$Cov(X,Y)= E[(X-\mu_X)(Y-\mu_Y)]$
- 特别的 $Cov(X,X)=Var(X)$
- $Var(\sum X_i)=\sum Var(X_i)+\sum_{i,j} 2Cov(X_i,X_j)$
- 这里一个意义说，投资股票时，期望等于分别收益的和，而方差能描述达到这个期望的波动状态，当无关时，就是两个的方差之和，
- 一个是离散的 看5x5的离散型的不同 概率情况
- 注意： 无关=> 协方差为0,但协方差为零不表示无关
	- 离散的例子 (X,Y)=(1,1)(2,4)(3,1) 概率都是1/3, 画成2维图更直观
		- E(X)=2， E(Y)=2,  Cov(X,Y)=E[(X-2)(Y-2)]=((1-2)(1-2)+(2-2)(4-2)+(3-2)(1-2))/3=0

协方差的意义？ - Charlie的回答 - 知乎 https://www.zhihu.com/question/23484126/answer/1601710771

离散情况的例子 https://www.bilibili.com/video/BV14A41197Uz

相关系数 $\rho=\frac{Cov(X,Y)}{\sigma_X\sigma_Y} \in[-1,1]$ 描绘的是 X,Y的线性相关性的 强度

---

偏斜度：三阶中心矩
- 测量了分布的不对称性，正态分布偏斜度是0
- 对于单峰的密度分布函数，可以看出哪一侧有较厚，长的尾巴
	- 偏斜度负的，那么 左侧尾巴比右侧更长

峰度：四阶中心矩
- 可以测量 正态分布的数据如何达到峰值和扁平的。
- 峰度较高会非常尖

在中心极限定理中，对于随机变量之和 如何收敛到正态分布这个问题，它们发挥了关键作用

### 10. 工具：卷积和变量替换

将在19章中看到，一个分布的矩会提供与该分布性质有关的线索。 这里会让我们引入卷积

$f_X(x)$ 在 -1/2~1/2 均匀分布

$f_Y(x)$ 在 -1/2~1/2 均匀分布

但是 我们 无法直接获得 $f_{X+Y}$  的分布情况，因为 不知道X和Y是否独立，有什么关系，
- X和X,那么 -1～1均匀分布
- X和-X, 那么就是 Prob(X=0)=1,变成“离散形状”
- X和Y不相关，-1~1 单峰f_0(z)=1 左右线性的分布函数

定义10.1.1 X,Y是定义在R上两个相互独立的连续型随机变量，他们概率密度函数分别是$f_X,f_Y$,卷积记作$f_X\star f_Y$, 表达式为

$(f_X\star f_Y)(z)=\int_{\infty}^{\infty}f_X(t)f_Y(z-t)dt$

离散型： $(f_X\star f_Y)(z)=\sum_n f_X(x_n)f_Y(z-x_n)$

性质：如果连续+独立，那么有 $f_{X+Y}(z)=(f_X\star f_Y)(z)$
- 作为学了傅里叶很多次，离散/连续
- 还学了生成函数，和各种其它卷积（例如狄利克雷卷积，min-max卷积）
- 那么卷积的感觉是什么呢， 新函数的 z处的值，是原来两个函数，自变量和=z 的值的乘积的和（积分），其中 这里（自变量和=z）在不同的 卷积要求下 可能是其它运算不是和
- 关于为什么是卷积，考虑二维的X,Y，再做45度 投影到一维还是很显然的

例子 掷骰子
- 卷积的性质，有交换律和结合律（原因主要是 卷积的运算符，是+具有交换性和结合性）
- 从而可以有多变量卷积

---

10.4.1 变量替换公式，设X是一个概率密度函数为$f_X$的连续型随机变量，并设存在一个区间$I\subset R$使得当$x\in I$时，$f_X(x)=0$ 换句话说，X只有在I中取值时，其概率密度函数才能不为0, 其中I可以是整个实直线。设$g:I\to R$是一个可微函数，$h=g^{-1}$,除了在有限多个点处的导数值可能为0外，g的导数在I中始终为正或者始终为负，如果令$Y=g(X)$ 那么
- $f_Y(y)=f_X(h(y)) |h'(y)|$
- I 良好区间
- 有一些表达式函数是可微的
- 要么严格增加，要么严格减少，希望每个X和唯一Y有关联
- 图像或链式求导法 $h'(y)=\frac{1}{g'(h(y))}$

例子： $f_{X}(x)=1/2,x\in[0,2]$ 其它部分为0, $g(X)=X^2$
- $I: [0,2]$
- 可微，导数g'=2x始终正
- $h(y)=\sqrt{y}$,$h'(y)=\frac{1}{2}\frac{1}{\sqrt{y}}$

$f_Y(y)=f_X(h(y))|h'(y)|=\frac{1}{2}y^{-1/2} f_X(\sqrt{y})=\frac{1}{4}y^{-1/2}$
- $\sqrt{y} \in I =[0,2]$ 所以$y\in[0,4]$
- 检验可以检验积分为1
- 注意 概率密度在0趋于无穷大（但是反常积分 并没有让积分结果有问题）

---

证明这个公式： 核心通过累积分布函数来证明的
- 注意导数正负 讨论，

---

随机变量的乘积与商，
- 写成微积分形式，
- 也就是中间的运算控制，最终还是落到 结果的z=运算(x,y), 那么其中积分的部分就是对应的

### 11. 工具: 微分恒等式

$\sum_{0}^\infty nx^{n-1}$ 中学阶段用的差分法，并且没有讨论收敛性

- 现在 可以用$\sum x^n$ 的导数，同时注意讨论收敛性
- $d/dx (\sum {x^n})=d/dx(\frac{1}{1-x})$


微分恒等式: $\sum_{n_{min}}^{n_{max}} f(n;a_1,\cdots,a_w)=g(a_1,\cdots,a_w)$ 其中f,g关于a可微，如果f退化到足以保证求和与求微分的次序可以交换，那么左右关于$a_i$ 的偏微分相等
- 这里依赖两个分析学结果：
	- 交换求和性质
	- 求导次序性质

---

例子：在二项分布随机变量上的应用
- Prob(X=k)=$\binom{n}{k} p^k (1-p)^{n-k}$ 其它为0

问题一：一阶矩 均值 $E[X]=\sum_{k=0}^n k\binom{n}{k}p^k(1-p)^{n-k}$

一个方法（这不就是之前b站和人battle的 二元偏导与1元偏导的东西吗，这还直接用了）
- $p\frac{\partial}{\partial p}(\sum_{k=0}^n\binom{n}{k}p^kq^{n-k})=p\frac{\partial}{\partial p}((p+q)^n)$, 先二元函数求偏导
- $p\sum_{k=0}^n\binom{n}{k}kp^{k-1}q^{n-k})=pn(p+q)^{n-1}$
- $\sum_{k=0}^n\binom{n}{k}kp^{k}q^{n-k})=np(p+q)^{n-1}$
- 有限和，可以交换求和顺序和偏导顺序
- 带入q=1-p
- $E[X]=np$ 妙啊 

问题二：二阶中心矩 方差 $Var(X)=E[X^2]-E[X]^2$, 那么问题是左边这个
- 那就二阶（多次偏导p）
- $E[X^2]=np+n(n-1)p^2$
- $Var(X)=np(1-p)$

---

例子 $X\sim N(\mu,\sigma^2)$ 表示X服从均值$\mu$,方差$\sigma^2$的正态分布（稍后定义？）
- 密度函数 $f_X(x)=\frac{1}{\sqrt{2\pi \sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$
- 具体正态在14章讲
- 标准正态分布$N(0,1)$
	- $M(k)=\int_{-\infty}^{\infty} x^k \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}} dx$

通过 概率密度积分总=1,和移动sigma 进行微分恒等

---

例子： 指数分布上应用
- $f_X(x)=\frac{1}{\lambda}e^{-x/\lambda}, x\ge 0$

我们同样 利用 概率密度积分总=1,和移动 lambda,进行微分恒等，来计算`E[X]`

## 第三部分 特殊分布

前面都是理论的框架结构弄好了，然后从这开始，研究特殊的分布，也就是“常见”至少“习题常见”的一些具体例子的性质

这类分布，要先验证表达式的合法性
- $\in [0,1]$的概率
- 总和为1的概率
### 12. 离散分布

#### 伯努利分布，0-1 分布

Prob(X=1)=p, Prob(X=0)=1-p, $X\sim Bern(p)$

$E[X]=p$

$Var(X)=E[X^2]-E[X]^2=p-p^2=p(1-p)$

#### 二项分布，n次独立0-1分布

$Prob(X=k)=\binom{n}{k}p^k(1-p)^{n-k}$, $X\sim Bin(n,p)$

$E[X]=np$

$Var(X)=np(1-p)$

在前面 微分恒等式中给过证明，在不相关变量的线性性质中也能证明E(X) 和 Var(X)

#### 多项分布

多项分布与多项式系数, n次选择，每次独立的k选1,每个选中概率$p_i$
- $\sum_i p_i=1$, 
- 概率密度函数的系数 $\frac{n!}{x_1!x_2!\cdots x_k!} \prod p_i^{x_i}$, 也就是n次选择中 选中第i个的次数$x_i$次,其中$\sum x_i=n$
	- 退化成 只有k=2个可选，就是 $\frac{n!}{x_1!x_2!}p_1^{x_1}p_2^{x_2}=\frac{n!}{t!(n-t)!}p^tq^{n-t}=\binom{n}{t}p^t(1-p)^{n-t}$

记作$X\sim Multinomial(n,k,p_1,\cdots,p_k)$

那么 对于其中一个 $X_i$ 可以把其余的都看成整体（非$X_i$), 那么有 $X_i\sim Bin(n,p_i)$
- $E[X_i]=np_i$
- $Var(X_i)=np_i)(1-p_i)$

那么 $E[X],Var(X)$呢，这里没有指标对于每个X_i, 所以

#### 几何分布 重复到成功的0-1分布

也就是 一直重复的独立事件，直到成功为止

$Prob(X=n)=p(1-p)^{n-1}$ 其中n是正整数

$X\sim Geom(p)$

$E[X]=\frac{1}{p}$, 微分恒等式或者，代回法

$Var(X)=\frac{1-p}{p^2}$

#### 负二项分布

r是正整数，随机变量X表示，恰好r次失败时，成功的次数

$Prob(X=k)=\binom{k+r-1}{k}p^{k}(1-p)^r$ 
- 这里注意的是“恰好”所以 最后一次一定要失败，那么前面k+r-1次中有k次成功
- $X\sim NegBin(r,p)$
- $E[X]=\frac{pr}{1-p}$
- $Var(X)=\frac{pr}{(1-p)^2}$

#### 泊松分布 巨大量实验只知道次数的 伯努利分布

$Prob(X=k)=\frac{\lambda^k e^{-\lambda}}{k!}$, n是正整数
- $X\sim Pois(\lambda)$
- $E[X]=\lambda$
- $Var(x)=\lambda$

不知道 $p,n$, 但有个 实际成功次数 常数$\lambda = pn$

$Prob(X=k)=\lim_{n\to \infty} \binom{n}{k}p^k(1-p)^{n-k}$

$=\lim_{n\to \infty} \binom{n}{k}(\frac{\lambda}{n})^k(1-\frac{\lambda}{n})^{n-k}$

$\displaystyle =\frac{\lambda^k}{k!} (\lim_{n\to \infty} \frac{n!}{(n-k)!n^k})(\lim_{n\to \infty} (1-\frac{\lambda}{n})^n) (\lim_{n\to \infty} (1-\frac{\lambda}{n})^{-k})$

$\displaystyle =\frac{\lambda^k}{k!} (1^k)(e^{-\lambda}) (1^{-k})$

$\displaystyle = \frac{\lambda^ke^{-\lambda}}{k!}$

#### 离散均匀分布

$Prob(X=a)=1/n$
- 如果 离散均匀分布的 特殊变量是 $a,a+1,\cdots, a+n-1$时，
	- $E[X]=a+\frac{n-1}{2}$, 
	- $Var(X)=\frac{n^2-1}{12}$

### 13. 连续型随机变量：均匀分布与指数分布

#### 均匀分布

$f_X(x)=\frac{1}{b-a},x\in [a,b]$
- $X\sim Unif(a,b)$
- 标准化$U\sim Unif(0,1)$
	- $X=(b-a)U+a$
	- $E[U]=1/2$
	- $Var(U)=1/12$
- $E[X]=\frac{a+b}{2}$
- $E[X]=\frac{(b-a)^2}{12}$
	
---

复合, $X\sim Unif(0,1),Y\sim Unif(0,1)$ 相互独立

$Z=X+Y$
- $f_Z(z)=z,z\in[0,1]$
- $f_Z(z)=2-z,z\in[1,2]$
- 一个性质 在 $Prob(1/2 \le Z \le 3/2) = 3/4$

概率论的一个核心结果是， “好”的独立的随机变量之和会收敛于正态分布

#### 指数分布

no. of events that occur in an interval of time

$\lambda =$ average no. of events in 1 unit of time

$\displaystyle f_X(x)=\frac{1}{\lambda}e^{\frac{-x}{\lambda}}, x\ge 0$, time taken between 2 events occurring
- $X \sim Exp(\lambda)$
- $E[X]=\lambda$
- $Var(X)=\lambda^2$
- 有些教材会用 $f_X(x)=\lambda e^{-\lambda x}$

---

n个 独立同分布的指数分布之和: 爱尔朗分布
- 广泛运用于排队论中
- $\displaystyle f_X(x)=\frac{x^{n-1}e^{-x/\lambda}}{\lambda^n(n-1)!}, x\ge 0$
- $E[X]=n\lambda$
- $Var(X)=n\lambda^2$
- 同样的 有些教材会是$\frac{1}{\lambda}$
- 这里 用图片说明了，随着n的增大，曲线越来越接近 正态分布

---

服从指数分布随机变量 例子与应用

球队直到下次进球的等待时间可以用指数分布逼近
- 参数30分钟(平均30分钟进一次球)的指数分布 $f_X(x)=\frac{1}{30}e^{-x/30}$, 上半场没进球的概率，也就是等待时间大于45分钟
	- $\int_{45}^\infty f_X(x) dx = -e^{-x/30}|_{45}^\infty=e^{-3/2}=0.223$ 约为

10个独立神经元，每个神经元12毫秒激活一次动作电位，每一个都激活一次，所需的时间之和，小于100毫秒 概率是？
- $\lambda=12,n=10$ 爱尔朗分布
- $\int_{0}^{100} \frac{x^{10-1}e^{-x/12}}{12^{10} (10-1)!}dx = 0.3255$ 约为

---

从指数分布中生成随机数

生成随机数的累积分布法： X随机变量，概率密度函数$f_X$,累积分布函数$F_X$, $Y\sim Unif(0,1)$
- $X=F_X^{-1}(Y)$
	- 称为逆变换抽样，或者逆变换法
	- $F_X$不减少，因此逆函数也是

例如指数分布$F_x(x)=1-e^{-x/\lambda},x\ge 0$
- $x=-\lambda \log(1-y)=F^{-1}_X(y)$

什么用呢？直接的采样一般会希望 样本的均匀，而逆变换采样希望概率的均匀

知乎上 一个例子是： 圆形内采样 https://zhuanlan.zhihu.com/p/622443806
- 方案1,包裹的正方形 随机长，随机宽采样，那么会有浪费（不属于圆的点）
- 方案2,随机半径，随机角度，会发现 靠近圆心的部分采样密集，远离圆心的部分采样稀疏
- 方案3,考虑半径的环分配密度，累积要是1, 且权重和环长有关（面积的导数，保证面积均匀）
	- 累积分布函数 $CDF=\frac{1}{R^2}x^2$
	- $CDF^{-1}(x)=R\sqrt{x}$

### 14. 连续型随机变量：正态分布

是最重要的分布之一，对于整个数学和科学领域都非常重要。主要因为中心极限定理（20章）

很多情况下，相互独立的随机变量之和都会收敛于正态分布，条件弱，许多理论和实际问题中都能得到满足

正态分布，高斯分布：如果随机变量X的概率密度函数是 $f_X(x)=\frac{1}{\sqrt{2\pi \sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$
- $E[X]=\mu$
- $Var(X)=\sigma^2$
- $X\sim N(\mu,\sigma^2)$
- 标准正态分布$X\sim N(0,1)$
- 图形上， $\mu$控制左右平移,$\sigma^2$控制有多尖锐

令$I(\mu,\sigma)=\int_{-\infty}^{\infty} f_X(x) dx$

$I(\mu,\sigma)=I(0,1)=1$
- 通过经典的，两个一元不相干积分乘积变成二维

---

稳定分布：
- X,Y都是独立同分布，如果X+Y的分布和X形状相同，那么称作稳定分布

服从正态分布的随机变量和之和：
- $X_i\sim N(\mu_i,\sigma_i^2)$是独立 的n个正态分布
- $X_1+\cdots+X_n\sim N(\mu_1+\cdots+\mu_n,\sigma_1^2+\cdots+\sigma_n^2)$
- 这个是个激动的结论，因为以前，只说任意能保证E(X)还是和，而独立能保证Var的和，而现在又多出了一个形状还是正态形状
- 记得 均匀分布的和变成三角形的单峰函数

---

$\Phi(x)=\int_{-\infty}^x \phi(x)=\int_{-\infty}^x \frac{1}{\sqrt{2\pi}}e^{-t^2/2}dt$

$\Phi(0)=1/2$

$\Phi(x)=1/2+\int_{0}^x\phi(t)dt$

$=1/2+\int_{0}^x\frac{1}{\sqrt{2\pi}}e^{-t^2/2}dt$

$=1/2+\int_{0}^x\frac{1}{\sqrt{2\pi}}\sum_n \frac{1}{n!}(-\frac{t^2}{2})^n dt$

$=1/2+\frac{1}{\sqrt{2\pi}}\sum_n \frac{(-1)^n}{n!2^n} \int_{0}^x t^{2n} dt$

$=1/2+\frac{1}{\sqrt{2\pi}}\sum_n \frac{(-1)^n}{n!2^n}\frac{x^{2n+1}}{2n+1}$

可以近似数值计算
- 前10项 误差 只有0.000 018 361

当数学家遇到一个无法求值的积分时，就会为该积分命名
- 误差函数 $Erf(x)=\frac{2}{\sqrt{\pi}}\int_{0}^xe^{-t^2}dt$
	- 方差为1/2 的正态分布的概率密度函数在0到x上的积分
	- $Prob(-2\le X\le 2)=Erf(\sqrt{2})$
	- $\Phi(x)-\Phi(-x)=Erf(\frac{x}{\sqrt{2}})$

---

例子与中心极限定理
- 通过 $X\to \frac{X-\mu}{\sigma}$ 来完成到$\Phi$的转换
	- $\mu=5,\sigma^2=16$
	- $Prob(2 < X < 7)=Prob(\frac{2-5}{4}<\frac{X-5}{4}<\frac{7-5}{4})=\Phi(1/2)-\Phi(-3/4)$

### 15. 伽马函数与相关分布

为什么要研究，很多分布，展开极限中会出现，有些看起来没出现的也要注意到$\Gamma(1/2)=\sqrt{\pi}$

若s>0(R(s)>0)那么 伽马函数$\Gamma(s)$

$\Gamma(s)=\int_{0}^{\infty}e^{-x}x^{s-1}dx=\int_{0}^{\infty}e^{-x}x^s\frac{dx}{x}$

通常积分容易出“问题”的是在零点和正负无穷大

性质证明：
- $\int_1^B e^{-x}x^{s-1}dx \le \int_1^{B} (M!x^{-M})x^{s-1}dx$
- $=M!\int_{1}^Bx^{s-M-1}dx$
- $=M!\frac{x^{s-M}}{s-M}|_1^B$
- $=\frac{M!}{s-M}[\frac{1}{B^{M-s}}-1]$

$\int_0^1e^{-x}x^{s-1}dx\le \int_{0}^1 1 \cdot x^{s-1}dx=\frac{1}{s}$

---

$\Gamma(1/2)=\sqrt{\pi}$

- 解析(亚纯)延拓
	- 做法要点是，给定定义在某区域上f,并把f的定义推广到更大的区域上，从而新函数g和f在f定义域上是一样的
	- 例子: 等比数列求和 $\sum_{i=0}^{\infty} r^i=\frac{1}{1-r}$, 这个左边在$|r|<1$有意义，右边在$r\neq 1$有意义，说明右边定义域比左边更广
		- 我们把右边称作左边的解析延拓
- 对于$\Gamma$ 可以证明$\Gamma(1)=1,\Gamma(n)=(n-1)\Gamma(n-1)$ 所以$n! = \Gamma(n-1)$,是阶乘的推广
	- 除了负整数和0以外，都有定义
	- 另一方面 可以用分析学研究 阶乘

$\int_{-\infty}^{\infty} e^{-x^2/2}dx=2\int_{0}^{\infty} e^{-x^2/2}dx$
- 令u=$x^2/2$有$x=(2u)^{1/2}$
- $=2\int_0^{\infty} e^{-u}(2u)^{-1/2}du$
- $=\sqrt{2}\int_0^{\infty} e^{-u}(u)^{1/2-1}du$
- $=\sqrt{2}\Gamma(1/2)$

余割等式，当s不是整数时 $\Gamma(s)\Gamma(1-s)=\pi csc(\pi s)=\frac{\pi}{sin(\pi s)}$

---

Beta函数与Gamma函数

$B(a,b)=\int_{0}^{1}t^{a-1}(1-t)^{b-1}dt$
- $a>0,b>0$时 $B(a,b)=\frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}$

Beta分布 $f_{a,b}=\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}t^{a-1}(1-t)^{b-1} dt,t\in[0,1]$
- $X\sim B(a,b)$
- 一种“更自由”的单峰函数生成器

$\Gamma(1/2)^2=\Gamma(1/2+1/2)\int_{0}^1 t^{1/2-1}(1-t)^{1/2-1} dt=\pi$

---

正态分布与Gamma函数

标准正态分布3个重要积分
- 概率积分=1
- $E[X] = 0$
- $Var(X)=1$

第四有用积分是2m阶矩
- $\mu_{2m}=\int_{-\infty}^{\infty} x^{2m}\phi(x)dx=(2m-1)!!$ 双阶乘的意思是，保持奇偶性隔一个取一个 `5!!=5 * 3 * 1, 6!!=6*4*2`
- $\mu_{2m}=\frac{2^m}{\sqrt{\pi}}\Gamma(m+\frac{1}{2})$

---

分布族(通过引入参数的不同，得到不同“形状”的分布)

伽马分布与韦布尔分布： 如果随机变量X的概率密度函数是
- $f_{k,\sigma}(x)=\frac{1}{\Gamma(k)\sigma^k}x^{k-1}e^{-x/\sigma} ,x \ge 0$
- X 服从正参数$k,\sigma$ 的伽马分布，k称为形状参数，$\sigma$称为尺度参数，记作$X\sim \Gamma(k,\sigma)$

韦布尔分布:
- $f_{k,\sigma}(x)=(k/\sigma)(x/\sigma)^{k-1}e^{-(x/\sigma)^k}$
- 被用在和许多涉及生存分析的问题上

---

余割公式

$\Gamma(s)\Gamma(1-s)=\int_{0}^{\infty} e^{-x}x^{s-1}dx \cdot \int_{0}^{\infty} e^{-y}y^{1-s-1} dy$

通过$x=r\cos \theta,y=r\sin \theta,dxdy=rdrd\theta$

$=\int_{0}^{\pi/2} \frac{ctan^{s-1}\theta}{ctan\theta +1} \frac{d\theta}{sin^2\theta}$

通过 $u=ctan\theta=\cos\theta/\sin\theta$

$=\int_{0}^{\infty}\frac{u^{s-1}}{u+1}dx$

---

柯西分布

上面有 $\Gamma(1/2)^2=\Gamma(1/2+1/2)\int_{0}^1 t^{1/2-1}(1-t)^{1/2-1} dt=\pi csc(\pi\cdot 1/2)=\pi$

$\pi = 2\int_{0}^{\infty} \frac{dz}{1+z^2}$

这里柯西分布密度函数 $f_X(x)=\frac{1}{\pi}\frac{1}{1+x^2}$
- 不存在均值 （不同趋近方式得到结果不同）
- 方差无穷大
- 在一些经济学理论中也起着重要的作用，随机游走假设的一个简单变体断言，
- 事实证明，柯西分布和正态分布（高斯分布），可以放在同一个具有不同参数的族中，两者都是 稳定分布(多个无关的随机变量线性组合还是同形状)

广义柯西分布:
- $\displaystyle f_{X}(x)=\frac{1}{b\pi}\frac{1}{1+\frac{(x-a)^2}{b^2}}$

### 16. 卡方分布

总是在假设检验中出现

$\displaystyle f(x)=\frac{1}{2^{v/2}\Gamma(v/2)}x^{v/2-1}e^{-x/2}, x\ge 0$
- 记作 $X\sim \chi^2(v)$
- 每选择一个$v$ 都会得到一个不同的分布
- 随着v正整数增加，突起向右移动
- 自由度：v, 正式定义为统计学中可以自由变化的数值个数，N个观测值，那么自由度通常是N-1或N
- 很多假设检验的检验统计量在原假设下服从卡方分布，适用于分类数据，
- 优点：它是一个非参数检验，意味着对数据总体分布没有任何假设
- 缺点：比其它参数检验统计效果差
- 卡方分布与正态分布之间的关联：$X\sim N(0,1)$，那么$X^2\sim \chi^2(1)$

![卡方分布](https://bkimg.cdn.bcebos.com/pic/0b7b02087bf40ad11e5f5a4e572c11dfa8eccee0?x-bce-process=image/format,f_auto/watermark,image_d2F0ZXIvYmFpa2UyNzI,g_7,xp_5,yp_5,P_20/resize,m_lfit,limit_1,h_1080)

---

特别的对于v=1,也就是$X\sim \chi^2(1)$
- $E[X]=1$
- $Var(X)=2$

---

卡方分布 与 服从正态分布的随机变量直和：设k是一个正整数，$X_1,\cdots,X_k$是相互独立且均服从标准正态分布的随机变量，这意味着 $X_i\sim N(0,1)$, 如果$Y_k=\sum_{i=1}^k {X_i}^2, Y_{k}\sim \chi^2(k)$$
- 再换句话说 ，独立的 卡方分布 的 随机变量之和 ，服从 $k=\sum k_i$ 的卡方分布

变量替换定理：V和W是R^k中有界开集，映射$h:V\to W$, 即是一一对应的，又是映上的
- $h(u_1,\cdots,u_k)=(h_1(u_1,\cdots,u_k),\cdots,h_k(u_1,\cdots,u_k))$
- 设$f: W\to R$ 是一个连续有界的函数
	- $\int\cdots\int_W f(x_1,\cdots,x_k)dx_1\cdots dx_k$
	- =$\int\cdots\int_V f(h(u_1,\cdots,u_k)) J(u_1,\cdots,u_k) du_1\cdots du_k$
		- 其中$J$是雅可比行列式，也就是 $J_{i行,j列}=\frac{\partial h_i}{\partial u_j}$ 偏导矩阵的行列式

## 第四部分 极限定理

### 17. 不等式和大数定律

如果 只知道 `E[X]`,不可能得到$Prob(X\ge a)$ 的 “不平凡”边界

马尔可夫不等式（基于全正+均值的基础信息），X是一个均值有限的非负随机变量，均值为$E[X]$(意味着 Prob(X< 0)=0), 那么对于任意正数a有
- $Prob(X\ge a)\le \frac{E[X]}{a}$
- 如果X是一个有单位量，那么左右都是纯数，很好，像概率
- $Prob(X\ge a)=\int_{x=a}^{\infty} f_X(x)dx$
- $=\int_{x=a}^{\infty} 1\cdot f_X(x)dx$
- $\le \int_{x=a}^{\infty} \frac{x}{a} \cdot f_X(x)dx$
- $=\frac{1}{a} \le \int_{x=a}^{\infty} x \cdot f_X(x)dx$
- $=\frac{1}{a} E[X]$

变形：X非负随机变量，均值$E[X]$有限,
- $Prob(X\ge lE[X])\le \frac{1}{l}$
- 这是我们利用“有限”信息得到的最好结果

---

切比雪夫不等式(Chebnyshev)
- 马尔可夫不等式，使用了一个输入来限制概率，即随机变量的均值，这是不够的，而且还要求随机变量非负
- 本章要一个更好的不等式（通过依靠更多信息）
- 根据 对于分布的研究，$E[X]$是一阶矩，Var(X)是二阶中心矩，很自然的希望能把基础信息增加一个 Var(X)
- X是随机变量,$\mu_X=E[X],\sigma^2_X=Var(X)$都是有限的，对于任意k>0有
- $Prob(|X-\mu_X|\ge k\sigma_{X})\le \frac{1}{k^2}$
	- 自然语言，(((随机变量 与 均值)的距离) 超过 (k倍 标准差) )的 概率 不超过$1/k^2$
- k 越大 ，限制的范围越小
- $=\int_{x:|x-\mu_x|\ge k_{\sigma_X}} 1\cdot f_X(x)dx$
- $\le \int_{x:|x-\mu_x|\ge k_{\sigma_X}} (\frac{x-\mu_X}{k\sigma_X})^2\cdot f_X(x)dx$
- $=\frac{1}{k^2{\sigma^2_X}} \int_{x:|x-\mu_x|\ge k_{\sigma_X}} (x-\mu_X)^2\cdot f_X(x)dx$
- $=\frac{1}{k^2{\sigma^2_X}}\int_{-\infty}^{\infty} (x-\mu_X)^2\cdot f_X(x)dx$
- $=\frac{1}{k^2}$
- 另一个好处是没有对于X要正的上定义的限制了

我觉得一点之前我卡住的点现在想通了
- 之前以为这个玩意能“消灭可能性”，然而这两个全是模型概率，并不能消灭可能性
- 可能之前有些书上写的是“准确度”，但是这个 词的 中文理解 出了 数学偏差

---

例子：正态分布上$X\sim N(0,1)$
- $|X|\ge 2,5,10$ 通过切比雪夫的得到的概率是 25%, 4%,1%
- 然而 通过积分得到的是 4.55%,0.0000573%,1.52x10^{-21}%
	- 注意到差距非常大
	- 因为 切比雪夫的条件很弱（只用了 EX Var(X)), 可能只是个很远的上界
- 又举例了$X\sim Unif(0,1),X\sim Exp(1)$
	- 都可以看到，马尔可夫更宽松，切比雪夫稍微近了一些，但都和实际的值有着差距

---

布尔Boole不等式，与邦弗伦尼Bonferroni不等式

把精确的容斥方法，截断：

布尔不等式：
- $Prob(\cup A_i)\le \sum Prob(A_i)$

Bonferroni 不等式：
- (截断到偶数个的$\sum$) $\le$ 真实概率 $\le$ 截断到奇数个的$\sum$
	- 这里的截断就是按照 容斥原理中，所选性质的个数 为指标
- 那么 Boole不等式，不过是 截断到1个时候的情况
- 用途就是，不需要完全计算，可以大约的得到一个上下界

---

弱大数定律和大数定律之前，描述不同的收敛类型

依分布收敛（弱收敛）： $X_i\cdots$ 是随机变量，对应累积分布函数$F_i\cdots$, C是F的连续点构成的实数集，
- 若$\lim_{n\to \infty} F_n(x)=F(x)$对于所有$x\in C$均成立，
- 那么随机变量序列$X_i\cdots$ 依分布收敛（弱收敛）
- 换句话说，如果F在x处是连续的，那么累积分布函数列在x处的极限就等于F(x)
	- 结论通常被记作$X_n\xrightarrow[]{d} X$或者$X_n\xrightarrow[]{D} X$
	- $X_n\xrightarrow[]{d}N(0,1)$ 表示收敛于一个服从标准正态分布的随机变量
- 例子：
	- 离散的 $f_n(x)=1/n, x\in${0,1/n,2/n,...,(n-1)/n}
	- 观察它的 CDF 是阶梯形状的函数
	- $\lim_{n\to \infty}F_n(x)=x$
	- $X_n \xrightarrow[]{d} Unif(0,1)$
- 感觉是 要点点收敛
	- 但是例子 $Prob_n(x=1/n)=1$ 的极限是 $F(x)=1, x>0$, 注意这里是>0而不是$\ge 0$,不是一个分布函数？（要求右连续？）
	- 那么希望的是 $F(x)=1,x\ge 0$, 所以这里弱收敛 抛开了间断点，比点点收敛要弱



---

依概率收敛: $X_i\cdots$ 都是随机变量，对于 任意$\epsilon > 0$ 都有
- $\lim_{n\to \infty} Prob(|X_n-X|\ge \epsilon)=0$
- 那么说依概率收敛到 $X_n\xrightarrow[]{p} X$, 或者 $X_n\xrightarrow[]{P} X$, 
- 常数分布：$\delta_c:$  Prob(X=c)=1, 
- $X_n\sim N(1234,1/n^2)$
	- 有结果$X_n\xrightarrow[]{p} \delta_{1234}$
	- 切比雪夫不等式 上场的时候了

---

需要分析学

几乎必然收敛：$(\Omega,F,Prob)$是一个概率空间，$X_i,\cdots$ 都是随机变量
- 如果 $Prob(\omega \in \Omega: \lim_{n\to \infty} X_n(\omega)=X(\omega))=1$
- 几乎必然收敛（或者几乎处处收敛，或者 以概率1 收敛于X）

必然收敛：$(\Omega,F,Prob)$是一个概率空间，$X_i,\cdots$ 都是随机变量
- 如果 $\lim_{n\to \infty} X_n(\omega)=X(\omega)$
- 必然收敛

---

弱大数定律与强大数定律

对于 随机变量$X_1\cdots X_n$,令$\bar{X_n}=\frac{\sum {X_i}}{n}$
- 中心极限定理（20.2.2） 指出， 适当假设下
- $Z_n=(\bar{X_n}-E[\bar{X_n}])/StDev(\bar{X_n})$ 会收敛于正态分布
- 假设 同分布，均值有限

弱大数定律：
- $X_i\cdots$ 独立同分布随机变量，均值$\mu$, $\bar{X_n}=\frac{\sum {X_i}}{n}$
	- 那么 $\bar{X_n}\xrightarrow[]{p}\mu$ 依概率收敛于$\mu$
	- 证明：
		- $E[\bar{X_n}]=\mu$
		- $Var(\bar{X_n})=\frac{\sigma^2}{n}$
		- 切比雪夫可以控制 到$\mu$
- 强大数定律：
	- 其它和 弱大数定律一致，前提都一致，
	- 区别是 弱大数定律是 依概率收敛，而强大数定律是几乎处处收敛

### 18. 斯特林公式

前面研究了 阶乘 与 gamma函数的关系

当n较大时，$n! \sim n^ne^{-n}\sqrt{2\pi n}$
- 或者说$\lim_{n\to \infty} \frac{n!}{n^ne^{-n}\sqrt{2\pi n}}=1$
- 再准确些 $n!=n^ne^{-n}\sqrt{2\pi n} (1+\frac{1}{12n}+\frac{1}{288n^2}\cdots)$


正反面抛硬币，2n次的期望次数是n,随着n增大也是$E[X]=n$ 但是另一方面，Prob(X=n) 趋于0
- 标准差是 $\sqrt{n/2}$ 意味着，均值“窗口”也在增长
- Prob(X=n)=$\binom{2n}{n}(1/2)^n(1/2)^n$
	- $约为 \frac{1}{\sqrt{\pi n}}$ 用斯特林公式替换掉前面的binom


引理 18.3.1 对于任意$\epsilon \le 1/9$, $|k|\le (2N)^{1/2+\epsilon}$ 时，如果$N\to \infty$, 那么有
- $(1+\frac{K}{N})^{N+\frac{1}{2}+k}(1-\frac{K}{N})^{N+\frac{1}{2}-k} \to e^{k^2/N}e^{O(N^{-1/6})}$
- 对 $log(1+x)$的泰勒展开
- $\binom{2N}{N+k}\frac{1}{2^{2N}}$ 约等于 $\frac{2}{\sqrt{2\pi (2N)}} e^{-(2k)^2/2(2N)}$
	- 独立同分布的泊松分布之和 服从 正态分布

---

较弱斯特林公式, $n\ge 3$
- $n^ne^{-n}e\le n!\le n^ne^{-n}en$
	- 证明 log n! 的展开 与 积分上下限

---

静态相位法：
 - 确定对于 Gamma函数定义，对于被积部分确定最大值，变成基于最大值的展开式
 - 展开式代换

### 19. 生成函数与卷积


服从泊松分布的随机变量之和： n个独立随机变量，分别服从泊松分布的系数$\lambda_i$
- 它们的和 服从 $\sum \lambda_i$的泊松分布

定义19.2.1 生成函数, $G_a(s)=\sum_{i=0}^{\infty} a_is^i$,其中s是使这个和收敛的任意数
- Fib为例:
	- $G_{fib}=\sum_{i=0}^{\infty} F_is^i$
	- $G=s+sG+s^2G$
	- G=$\frac{s}{1-s-s^2}$
	- $F_n=\frac{1}{\sqrt{5}}(\frac{1+\sqrt{5}}{2})^n-\frac{1}{\sqrt{5}}(\frac{1-\sqrt{5}}{2})^n$ 比内公式
	- 这里还给了矩阵乘法 特征根的 内容
- 定理19.3.1 序列生成函数的唯一性
	- 在$|s| < \delta$ 使得序列收敛，两个序列系数对应相等 当且仅当 在 收敛范围内生成函数带入后值全部相等
- 离散型随机变量的卷积：
	- 序列卷积
	- 一个重要结论 生成函数 展开后 互相卷积的结果的生成函数 = 两个原函数的乘积
		- 这在简化数学运算 很有用，一些算法竞赛里也很有用
		- $G(s)=G_a(s)G_b(s)$

注意到之前的 随机变量之和的过程 是卷积
- 这里定义概率生成函数, 对于离散型
- $G_X(s)=\sum_{m=-\infty}^{\infty} s^m Prob(X=m)$
- 那么有了 结论 针对 独立 离散变量
- $G_{\sum X_i}(s)=\prod G_{X_i}(s)$
	- 独立离散随机变量，和的概率密度函数 是 各个随机变量概率的卷积

---

连续型

概率生成函数$G_X(s)=\int_{-\infty}^{\infty} s^xf(x)dx$

例子 指数生成函数 $G_X(s)=\int_0^{\infty} s^x \frac{1}{\lambda} e^{-x/\lambda} dx=\frac{1}{1-\lambda \log s}$

函数卷积 $f\star g(x)=\int_{-\infty}^{\infty} f(t)g(x-t)dt$
- $G_{f\star g}=G_{g\star f}=G_f\cdot G_g$

---

矩母函数的定义与性质

首先 $G_a(s)=\sum_{i=0}^{\infty}a_is^i$, 反过去有 $a_t=\frac{1}{t!}\frac{d^t}{ds^t}(G_a(s))$
- 矩母函数 不含 $\frac{1}{t!}$这样的东西

定义19.6.1 矩, X是一个概率密度函数为f的随机变量，
- 如果X是离散的，并且仅当取$x_m$时概率不为0,那么它的k阶矩$\mu_k'$被定义为
	- $\mu_k'=\sum_{i=0}^{\infty} x_i^kf(x_i)$
- 如果X是连续的
	- $\mu_k'=\int_{-\infty}^{\infty} x^kf(x)dx$
- $\mu_k'= E[X^k]$
- k阶中心矩$\mu_k=E[(X-\mu_1')^k]$

19.6.2 矩母函数:
- $M_X(t)=E[e^{tX}]=G_X(e^t)$
	- $G_X(s)=M_X(\log s)$
- 离散的 $=\sum_{i=-\infty}^{\infty} (e^{t})^{x_i}f(x_i)$
- 连续的 $=\int_{-\infty}^{\infty} e^{tx}f(x)dx$
- 也就是 还是卷积的思想，不过 把$s$写作$e^t$

19.6.3 X随机变量, 矩$\mu_k'$
- $M_X(t)=1+\mu_1't+\frac{\mu_2't^2}{2!}+\frac{\mu_3't^3}{3!}+\cdots$
	- $\mu_k'=d^k/dt^k (M_X(t)) |_{t=0}$
- a,b是常数
	- $M_{aX+b}(t)=e^{bt}M_X(at)$
- 独立变量:
	- $M_{\sum X_i}(t)=\prod M_{X_i}(t)$

19.6.5 离散型随机变量矩母函数唯一性
- X,Y 取非负整数时 概率才不为0,离散型随机变量，矩母函数在 $|t|<\delta$ 时收敛
- X和Y同分布 <=> 存在r>0 矩母函数相等

然而 对于连续型并没有这么美好
- 存在不同的概率分布，有相同的矩
	- $f(x)=\frac{1}{\sqrt{2\pi x^2}}e^{-(\log^2 x)/2}$
	- $g(x)=f(x)[1+\sin(2\pi \log x)]$
	- 函数图像明显的差异
	- 再来 $h(x)=e^{-1/x^2}$ 它在0的任意阶导数都是0
- 离散型的生成函数是唯一的，但连续型的不是
	- 参阅 复分析与中心极限定理

### 20. 中心极限定理的证明

中心极限定理(CLT) 概率论中真正的瑰宝之一
- 假设弱：条件容易满足
- 结果普遍性

如果存在一些独立同分布的“好”随机变量$X_i$ 并且它们的均值是$\mu$ 且标准差是$\sigma$, 
- 那么标准化随机变量$Z_n=\frac{(\sum X-i)-n\mu}{\sigma \sqrt{n}}$, $E[Z_n]=0,Var(Z_n)=1$
- 会收敛于 N(0,1), 如何收敛（会取决于假设）
- $M_X(t)=E[e^{tX}]=\int_{-\infty}^{\infty} e^{tx}f_X(x)dx=\sum_{n=0}^{\infty} \frac{\mu_n't^n}{n!}$
- $M_X(0)=1$ 
	- 大多数时候 0的邻域内存在， $\delta >0, |t| < \delta$时$M_X(t)$收敛，（柯西分布不存在）
- f,g连续，对于所有h满足 int hf = int hg,那么f,g相等
	- 反证 如果某点不等，通过平移切割可以切割出一个邻域相差足够量的值， 对应区间可以取值，其它区间全0，可以使得 左右积分足量不等
	- 但这个性质使用上难点是如何尽量少和尽量容易计算的h的选取来完成“测试”

---

正态分布 高斯分布：$f(x)=\frac{1}{\sqrt{2\pi \sigma^2}}e^{-(x-\mu)^2/(2\sigma^2)}$

CLT中心极限定理： 独立同分布随机变量 $X_i$, 矩母函数$M_X(t)$, 在领域$|t| < \delta$中收敛
- $\bar{X_N}=\frac{X_1+\cdots+X_N}{N}$
- $Z_N=\frac{\bar{X_N}-\mu}{\sigma / \sqrt{N}}$
- 那么$N\to \infty$时，$Z_N$收敛于$N(0,1)$
	- 基本形状决定了正态分布的收敛速度 贝里-埃森定理

---

- E[X] 均值
- Var(X) 越大，概率密度函数分布越分散
- 标准差 $StDev(X)=\sqrt{Var(X)}$, 对于单位系统 是同单位的

例子：
- 同样学生，同样内容，两个试卷
- 试卷1 94分，但是班级 E[X]=97 StDev(X)=1
- 试卷2 84分，但是班级 E[X]=64 StDev(X)=10
- 会认为第二个更优秀

随机变量标准化 $Z=\frac{X-E[X]}{StDev(X)}=\frac{X-\mu}{\sigma}$
- 从而 $E[Z]=0,Var(Z)=1,StDev(Z)=1$

---

矩母函数相关结果
- 正态分布$X\sim N(\mu,\sigma), M_X(t)=e^{\mu t+\frac{\sigma^2t^2}{2}}$
	- 特别的$X\sim N(0,1),M_X(t)=e^{t^2/2}$


20.5.4 矩母函数收敛 -> CDF 累积分布函数由 矩母函数给出，且 CDF收敛
- 建立了 同时收敛的 的关系

---

例子： 泊松分布 $Prob(X=k)=\frac{\lambda^k e^{-\lambda}}{k!}$, n是正整数
- $X\sim Pois(\lambda)$
- $E[X]=\lambda$
- $Var(x)=\lambda$

$M_X(t)=1+\mu t+\frac{\mu_2't^2}{2!}+\cdots = e^{\lambda(e^t-1)}$
- $\mu=\lambda$一阶导,令t=0
- $\mu_2'=\lambda+\lambda^2$ 2阶导,令t=0
	- $\sigma^2=E[X^2]-E[X]^2=\lambda$
- $E[\bar{X}]=\lambda$
- $Var(\bar{X})=\frac{\sigma^2}{N}=\frac{\lambda}{N}$
- $Z_n=\frac{\bar{X_N}-E[\bar{X_N}]}{StDev(\bar{X_N})}$
	- $=\frac{\sum X_i-N\mu}{\sigma\sqrt{N}}$
- $M_{Z_n}(t)=M_{\sum \frac{X_i-\mu}{\sigma\sqrt{N}}}(t)$
- $=\prod_N M_{\frac{X_i-\mu}{\sigma\sqrt{N}}}(t)$
- $=(M_{\frac{X_i-\mu}{\sigma\sqrt{N}}}(t))^N$
- $=(e^{\frac{-\mu t}{\sigma \sqrt{N}}} M_X(\frac{t}{\sigma\sqrt{N}}))^N$ 矩母函数 中随机变量线性变换时的性质 19.6.3
- $=(e^{\frac{-\mu t}{\sigma \sqrt{N}}} e^{\mu (e^{\frac{t}{\sigma\sqrt{N}}}-1)})^N$ 使用 泊松分布的矩母函数
- $=e^{\frac{-\mu t \sqrt{N}}{\sigma}} e^{\mu N (e^{\frac{t}{\sigma\sqrt{N}}}-1)}$ 使用 泊松分布的矩母函数
- $=e^{\frac{-\mu t \sqrt{N}}{\sigma}} e^{\mu N (\frac{t}{\sigma\sqrt{N}}+\frac{1}{2}(\frac{t}{\sigma\sqrt{N}})^2+O(\frac{t^3}{N\sqrt{N}}) )}$ 带O的 泰勒展开
- $=e^{\mu N (\frac{1}{2}(\frac{t}{\sigma\sqrt{N}})^2+O(\frac{t^3}{N\sqrt{N}}) )}$刚好第一项抵消
- $=e^{\frac{\mu t^2}{2\sigma^2}+O(\frac{t^3}{\sqrt{N}} )}$ 展开
- $=e^{\frac{t^2}{2}+O(\frac{t^3}{\sqrt{N}} )}$ 带入具体的mu和sigma

N足够大，那么就是趋于 正态分布的矩母函数，从而CDF趋于正态分布
- 其中大O的部分 控制了收敛速度

---

更一般的 $\displaystyle M_{Z_N}(t)=\prod_{n=1}^N e^{-\frac{\mu t}{\sigma \sqrt{N}}} M_X(\frac{t}{\sigma \sqrt{N}}) = e^{-\frac{\mu t \sqrt{N}}{\sigma}} (M_X(\frac{t}{\sigma \sqrt{N}}))^N$
- 上面的过程在这里 具体带入了$M_X$
- 而如果不知道具体的$M_X$呢

$\displaystyle \log M_{Z_N}(t)= -\frac{\mu t \sqrt{N}}{\sigma} + N \log M_X(\frac{t}{\sigma \sqrt{N}})$

因为 $\log M_X(t)= \log (1+\mu t+\frac{\mu_2't^2}{2!}+\cdots)$
- $=\log (1+t(\mu+\frac{\mu_2't}{2!}+\cdots))$
- $=t(\mu+\frac{\mu_2't}{2!}+\cdots)) - \frac{t(\mu+\frac{\mu_2't}{2!}+\cdots))^2}{2} + \cdots$, 对于 log(1+x)展开
- $=\mu t + \frac{\mu_2'-\mu^2}{2}t^2+ t^3$或更高次项
- $=\mu t + \frac{\sigma^2}{2}t^2+ t^3$或更高次项


所以 $\displaystyle -\frac{\mu t \sqrt{N}}{\sigma} + N \log M_X(\frac{t}{\sigma \sqrt{N}})$

$\displaystyle = -\frac{\mu t \sqrt{N}}{\sigma} + N (\frac{t}{\sigma \sqrt{N}}+\frac{\sigma^2}{2}(\frac{t}{\sigma \sqrt{N}})^2+(\frac{t}{\sigma \sqrt{N}})^3项或N次数更低项)$

$\displaystyle = \frac{t^2}{2} + O(N^{-1/2})$

而什么时候能抛弃O,也就是 在 0的领域内能收敛

---

积分学最大的谎言之一就是你可以求出积分，具体说可以找到漂亮表达式

中心极限定理 重要应用之一：蒙特卡罗法
- 以极高精度确定高维积分
- 最早用途是试图了解如何制造原子弹

核心思想：
- 面积 约为 $\frac{落在区域内个数}{总次数} * 包裹面积$，包裹可以用长方形正方形方便计算
- 概率论性质，随着样本增大$\bar{X_N}$ 趋于 $E[面积]=实际面积, Var[面积] = \sqrt{面积(1-面积)}/\sqrt{N}$ 的正态分布，也就是方差在不断缩小

### 21. 傅里叶分析与中心极限定理

引子: $f(x)=\frac{C}{1+x^8}$,显然 积分有限，选取合适的C可以变成概率密度函数
- 但显然>=8偶阶矩无穷大，所以矩母函数不存在，原点附近不收敛，所以满足前面中心极限定理的条件
- Mandelbrot关于金融和大宗商品市场分形特性的研究中，柯西分布就会出现
- 虽然 独立同分布 柯西分布不会趋于 正态分布
- 但是这个f可以

剧透：核心是 有限的 `E[X],Var(X)`
- 方法是研究密度函数的傅里叶变换，概率论中被称为特征函数
- 与矩母函数不同，特征函数总是存在（存在性 更好）

构造函数到函数的映射 $(\mathcal{K}f)(y)=\int_I f(x)K(x,y)dx$

特别的$K(t,s)=e^{-ts}$记作拉普拉斯变换
- $(\mathcal{L}f)(x)=\int_{0}^{\infty} f(t)e^{-tx}dt$
- 逆变换 $(\mathcal{L^{-1}}g)(x)=\lim_{T\to \infty}\frac{1}{2\pi i} \int_{c-i T}^{c+i T} e^{tx}g(t)dt$
	- 逆变换 $(\mathcal{L^{-1}}g)(x)=\lim_{T\to \infty}\frac{1}{2\pi i} \int_{-T}^{T} e^{(c+ti)x}g(c+ti)idt$


特别的$K(t,s)=e^{-2\pi i ts}$记作傅里叶变换
- $(\mathcal{F}f)(y)=\hat{f}(y)=\int_{-\infty}^{\infty} f(x)e^{-2\pi i xy}dx$
- 逆变换$(\mathcal{F^{-1}}f)(y)=\int_{-\infty}^{\infty} f(x)e^{2\pi i xy}dx$

---

定义21.1.3 施瓦兹空间， $\mathcal{S}(\mathbb{R})$ 是全体满足下列条件的 无限可微函数f构成的集合，对于任意的非负整数m,n有
- $\text{sup}_{x\in\mathbb{R}}|(1+x^2)^m \frac{d^n}{dx^n} (f) | < \infty$, 上确界有限
- 例如 高斯分布 导数是 $p_n(x)\frac{1}{\sqrt{2\pi}} e^{-x^2/2}$, 
	- $|x^m e^{-x^2/2}|$ 有界 -> 满足要求

复分析结果：当积分变换来自于唯一输入时，其陈述是精确的

定理21.1.4反演定理， $f\in \mathcal{S}(\mathbb{R})$，那么
- $f(x)=\int_{-\infty}^{\infty} \hat{f}(y) e^{2\pi i xy}\text{d}y$
	- 这句白话讲就是 在施瓦兹空间中的函数的傅里叶变换的逆变换得到它自己
	- 特别的 如果f,g都属于 施瓦兹空间，且傅里叶变换相同，那么f=g

作为对比
- 矩母函数 $M_X(t)=E[e^{tX}]$
- 特征函数 $\phi_X(t)=E[e^{itX}]$
	- 如果X的概率密度函数是连续的，那么对于所有的t,特征函数始终存在
	- 本质上是概率密度的傅里叶变换 $\phi(-2\pi t)$

为什么复分析的结果可以带来这么大的帮助，反演公理告诉我们，如果初始分布很好，那么知道函数的积分变换就等于知道了这个函数，积分变换可以唯一地确定分布

对于函数: $f: \mathbb{R}\to \mathbb{C}$，如果存在一个有限的闭区间[a,b],使得当$x \not\in [x,y]$时 $f(x)=0$,那么我们说f有 紧支集 [a,b].
- 定义在有限闭区间[a,b]上的连续函数g, 存在一个具有紧支集的 施瓦兹函数f,使得f与g能够任意的接近(也就是 $x\in[a,b],|f(x)-g(x)|<\epsilon$)
- 类似地 任意给定一个这样的连续函数g,按照上述思路可以找到一系列定义在区间上的阶梯函数之和，并且任意接近于g
- 施瓦兹函数是无限可微的

柯西-施瓦兹不等式：对于复值函数f,g:
- $\int_{-\infty}^{\infty} |f(x)g(x)|dx\le (\int_{-\infty}^{\infty} |f(x)|^2dx)^{1/2} (\int_{-\infty}^{\infty} |g(x)|^2dx)^{1/2}$
	- 这要求平方可积，大多数概率密度函数都能满足这一点


21.2.1 卷积与傅里叶变换,f,g都是$\mathbb{R}$上的连续函数， 如果f,g平方积都是有限的，那么$h=f \star g$存在，且$\hat{h}=\hat{f}\hat{g}$
- 卷积结果 的 傅里叶变换 = 傅里叶变换 的 对应位置乘积


引理21.2.2 ,独立 平方可积的和复合结果 为它们的卷积

---

基于 傅里叶变换 的 中心极限定理证明

定理 21.3.1 $X_i$独立同分布，前3阶矩有限，且概率密度函数的衰减速度“足够快”，
- $Z_{N\to infty}=\frac{\bar{X_N}-\mu}{\sigma/\sqrt{N}}$ 收敛于标准正态分布$N(0,1)$

## 第四部分 其它主题

### 22.  假设检验

#### z检验 (稳定，中心极限定理， >=30个样本， 正态分布检验)

也就是假设模型，检验和模型的契合程度，或者模型的参数

通常建立 原假设
- 通常与 实验员或研究者试图证明的结论相反
- 我们希望 假设原假设正确 用数据推翻它 （像一种无罪推断）
- 备选假设 与原假设互补
- 在原假设下，如果收集到现在数据的概率最够小。拒绝原假设，接受互补假设
- 需要指定一个显著水平限度，$\alpha$水平，也就是说，在原假设下， 小于显著水平，那么就拒绝原假设，例如0.05,
	- 你应该在看具体数据 之前 设定边界点（显著水平）
	- 例如 $N(\mu,\sigma)$的0.05显著水平下，范围是$\mu-1.96\sigma,\mu+1.96\sigma$ 
		- 这是双侧检验
			- 产品寿命检验，  寿命+-
		- 单侧检验的变动值是 $\mu + 1.64\sigma$
			- 麦当劳配餐速度检验 < t 或者 >=t
	- 对于更重要的事情可能选择更小的显著性水平


检验统计量，数据越多越好，方差越小，两倍数据方差减半
- 之前结论$\bar{X}\sim N(\mu,\sigma^2/n)$ 随着n增大

样本均值的检验统计量——z统计量
- $X$服从方差$\sigma^2$的正态分布，假设其均值为$\mu$,
	- 如果$X$是服从某个“良好性质”分布, n>=30时，$\bar{X}$会近似于服从正态分布
- $x_i$是从该分布中出取出的n个相互独立的观测值。$\bar{x}$为样本均值
- $z=\frac{\bar{x}-\mu}{\sigma / \sqrt{n}}$ 称作观测到的z检验统计量
- $Z\sim N(0,1)$
- 称作z,是因为稳定的

#### p检验 （成立指标）

p值让我们了解到收集的数据远离已有值的概率
- p值是条件概率，原假设成立的前提下，收集到已有数据的概率
- 上下文有关
- 对于相同数据的不同检验可以产生不同的p值
- p值不是原假设成立的概率


抛硬币20次，12次正面,
- 原假设 硬币均匀
- p值= 出现12~20次正面概率和 + 出现0～8次正面概率和 = 0.503
- 不接受，只拒绝或者无法拒绝

- 如果原假设成立，收集到已有证据的条件概率
- 而不是在已经收集了数据的前提下，原假设成立的概率
- 样本太小 也不会用正态检验，有的直接二项分布检验


#### t检验 （量少，不知道方差）

- z检验要我们明确知道方差（模型），但很多时候做不到这一点
- 可以从数据中得到方差的估计值，并用它来替代实际方案
- 利用样本方差来改变检验统计量的分布

估计样本方差$s^{2}=\frac{1}{n-1}\sum_{i=0}^n (x_i-\bar{x})^2$
- 这里$n-1$基于统计学的 自由度 概念，对于任意一个给定的估计，自由度是作出该估计需要的独立观测值的个数
- 另一个说法 $E[s^2]=\sigma^2$ 无偏估计量

t分布，是一个分布族，参数是自由度，$X_i\sim N(0,1)$相互独立随机变量$S_n^2$是样本方差随机变量
- 那么$\bar{X}/{S_n/\sqrt{n}}$ 称为自由度n-1的t分布，记作$T_{n-1}$
- 对于自由度$v$的t分布，概率密度函数的解析表达式$\frac{\Gamma(\frac{v+1}{2})}{\sqrt{v\pi}\Gamma(v/2)}(1+\frac{x^2}{v})^{-\frac{v+1}{2}}$
- 如果基本分布被推测均值是$\mu$的正态分布,方差未知
	- 检验统计量$\frac{\bar{x}-\mu}{s/\sqrt{n}}\sim t_{n-1}$
	- t分布在n趋于无穷大时，趋于正态分布
- 因为信息更少（不知道方差，所以比z检验更难拒绝）， 但适用范围更广

---

- 概率论证 从来都不是决定性的
- 我们可能会犯两种错误
	- 拒绝一个正确的原假设
		- 恰好是$\alpha$水平
	- 无法拒绝一个不正确的原假设
		- 原假设与事实非常接近

---

#### 检验方差

目前只检验了均值，接下来如何对方差进行测试

- 服从正态分布的随机变量与服从卡方分布的随机变量之间有一定的关联
- 这种关联引出了样本方差与t检验
- 我们可以利用服从卡方分布的随机变量来检验理论和实验的拟合优度

卡方分布$X\sim N(0,1),X^2\sim \chi^2(1)$
- 概率密度函数 $1/\sqrt{2\pi}x^{-1/2}e^{-x/2}$
- $Y=\sum_n X_i^2 \sim \chi^2(n)$
	- 概率密度函数 $\frac{1}{2^{k/2}\Gamma(k/2)}x^{k/2-1}e^{-x/2}$
- 服从正态分布的随机变量，其样本方差与$\chi^2$分布密切相关

$\sum_{i=1}^{n}(\frac{x_i-\mu}{\sigma})^2=\frac{1}{\sigma^2}\sum_{i=1}^n(x_i-\bar{x})^2+(\frac{\bar{x}-\mu}{\sigma/\sqrt{n}})^2$

$\chi^2(n)=()+\chi^2(1)$

$\frac{(n-1)s^2}{\sigma^2}\sim \chi^2_{n-1}$


---

用  卡方分布定义 t分布
 
自由度为 k的t分布， $X\sim N(0,1)$, $Y_n \sim \chi_n^2$
- $\frac{X}{\sqrt{Y_n/n}}\sim t_n$
- $t_{n-1} \sim \frac{\bar{x}-\mu}{s/\sqrt{n}}=\frac{\bar{x}-\mu}{\sigma/\sqrt{n}}\frac{1}{\sqrt{\frac{1}{n-1}\frac{(n-1)s^2}{\sigma^2}}}=X\frac{1}{\sqrt{\frac{1}{n-1}Y_{n-1}}}$

---

列表数据过拟合，检验

$\chi^2_{k-1}=\sum_{i=1}^k \frac{(O_i-E_i)^2}{E_i}$
- 第i个结果出现观测次数：$O_i$
- E_i在原假设下 预期观测到次数

---

双样本检验
- 对两个总体进行检验，它们方差已知但互不相同
- 对两个总体进行检验，它们方差未知但是相等，合并（样本）房颤，对未知房颤的加权估计
- 对于方差未知，且互不相同的两个总体，我们可以利用近似来处理

知道方差，构造 $\bar{X}-\bar{Y}$, 如果相互独立
- $Var(\bar{X}-\bar{Y})=Var(\bar{X})+Var(\bar{Y})=\frac{\sigma_X^2}{n_X}+\frac{\sigma_Y^2}{n_Y}$
- $E(\bar{X}-\bar{Y})=\mu_X-\mu_Y$
- $z=\frac{(\bar{X}-\bar{Y})-(\mu_X-\mu_Y)}{\sqrt{\frac{\sigma_X^2}{n_X}+\frac{\sigma_Y^2}{n_Y}}} \sim N(0,1)$

未知方差，但相等
 - $s_p^2=\frac{(n_x-1)s_x^2+(n_y-1)s_y^2}{n_x-1+n_y-1}$
	 - 令$r=\frac{n_x-1}{n_x-1+n_y-1}$有$s_p^2=rs_x^2+(1-r)s_y^2$
- $\frac{(\bar{X}-\bar{Y})-(\mu_X-\mu_Y)}{\sqrt{\frac{s_p^2}{n_X}+\frac{s_p^2}{n_Y}}} \sim t_{n_x+n_y-2}$

未知方差，不相等
- $\frac{(\bar{X}-\bar{Y})-(\mu_X-\mu_Y)}{\sqrt{\frac{s_x^2}{n_X}+\frac{s_y^2}{n_Y}}} 约为 t_v$

$v=\frac{(\frac{s_x^2}{n_x}+\frac{s_y^2}{n_y})^2}{\frac{s_x^4}{n_x^2(n_x-1)}+\frac{s_y^4}{n_y^2(n_y-1)}}$



### 23. 差分方程、马尔科夫过程和概率论

翻倍加一，看起来“让最终成功”的概率很大，但每次的风险是低收益，高损失可能的
- 例如你最多连续下5次
- 那么 最终失败的代价是$2^{64}$每次成功的收益是1
- 而 实际上你面对的概率是 n次连续操作中，出现连续失败长度 >=5的概率
	- 100次时 81.01%
	- 200次时 96.59%
- fib数列 趋于 $\frac{1}{\sqrt{5}}(\frac{1+\sqrt{5}}{2})^n$
	- 来自递推数列的启发
	- 上面也可以递推算出
- 概率 估计 100切成20组每组5个，可以确定一个下界

线性递推函数
- 特征方程的根，特征值

---

马尔可夫过程
- 群动力学
- 莱斯利矩阵
- 随机矩阵论
- 马尔可夫过程：只需要n时刻状态，已知变化规律（线性？），可以推出n+1时刻状态

### 24. 最小二乘法

拟合到一条直线上

我们几乎不可能观察到完美的线性关系

定义误差函数 E(a,b)=$\sum_{i=1}^N (y_i-(ax_i+b))^2$

最值 $\nabla E(a,b)=(\frac{\partial E}{\partial a},\frac{\partial E}{\partial b})=(0,0)$
- $\sum_{i=1}^n (y_i-(ax_i+b))x_i=0$
- $\sum_{i=1}^n (y_i-(ax_i+b))=0$

### 25. 婚姻秘书问题，蒙提霍尔问题

秘书问题
- n个申请人，每人面试一次，立即决定录用与否，不录用则失去机会，什么策略选到最佳候选人的概率最大
- 我们假设所有人有排名（均匀离散）
- 要录用排名第一的人的概率最大，而不是我们要录用的人的 排名概率加权的期望最大，数学模型简单，但和现实更不符合
- 面试次序等概率。

Prob(成功)=\sum Prob(成功|第i个是最优秀) * Prob(第i个是最优秀的)
 - 期望 基础=前k个人最大值，然后 首次遇到超过这个值得就录用，那么需要 最优的人前面的第二优的人在前k人中
 - 在这个模型策略下 k 约为 n/e

---

蒙提霍尔问题
- 3门，开一门，是否换门的后验概率问题


## 碎片

随想

1. 有习题，没习题答案，习题远超课程讲的
2. 我觉得 这本和之前微积分lifesaver都可以算“导读”类的书，一些细微的，需要教科书，更需要习题册，甚至我觉得带有答案的习题册更重要
3. 这本书很多专业名词首次出现时没有英文
4. 这本书还是有不少公式是 drop from sky的
5. 这书的符号 有的时候还是有些混乱，一会i一会n一会m的
6. 这里不同分布之间的关联讲得很有用
	1. 这书的脉络 联系上是真不错，例子还是不够，非数学的外部带入感感觉还有可以加入的东西，例如我有些没理解的知乎搜的一些相关解释

外链

- [概率论中，「矩」（moment）的实际含义是什么？高阶矩表示数据的哪些状态？](https://www.zhihu.com/question/23236070)
- [随机变量的矩和高阶矩有什么实在的含义？](https://www.zhihu.com/question/25344430)
- [统计学中「矩」这个概念是怎么引入的？它为什么被称为矩？它与物理意义上的矩有什么相同与不同？](https://www.zhihu.com/question/19915565)
- [概率收敛、均方收敛、分布收敛、几乎处处收敛区别与联系的直观解释？](https://zhuanlan.zhihu.com/p/27468615)